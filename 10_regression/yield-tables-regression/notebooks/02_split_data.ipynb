{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f68a806",
   "metadata": {},
   "source": [
    "# Split data\n",
    "\n",
    "In this notebook, we split the data into training and testing sets. The result will be four datasets: `X_train`, `X_test`, `y_train`, and `y_test`, which will be stored in `../data/interim/`. Those datasets will be used for training and evaluating the machine learning model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4f5499e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import RobustScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "# Load the cleaned up data\n",
    "df = pd.read_csv(\"../data/interim/cleaned_data.csv\")\n",
    "\n",
    "target_variable = \"volume_per_ha\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b835a37",
   "metadata": {},
   "source": [
    "## Handle categorical features\n",
    "\n",
    "There are some categorical features on the dataset like the `id` (refers to a tree species) and the `yield_class`. Fortunately, they are both already numeric values, so we don't need to apply any encoding.\n",
    "\n",
    "The following code snippet will not do anything - but we keep it in in case we add other categorical features in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4e8dafb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = []\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df[categorical_features] = df[categorical_features].apply(label_encoder.fit_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8923d959",
   "metadata": {},
   "source": [
    "## How to split the data\n",
    "\n",
    "As the data is no time series data, we can use a simple train-test split. We will use 80% of the data for training and 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72f4c0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 3314\n",
      "Test set size: 1105\n"
     ]
    }
   ],
   "source": [
    "# Define features and target\n",
    "X = df.drop(columns=[target_variable])\n",
    "y = df[target_variable]\n",
    "\n",
    "# Split into train and test sets (e.g., 80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "print(\"Training set size:\", X_train.shape[0])\n",
    "print(\"Test set size:\", X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382d033a",
   "metadata": {},
   "source": [
    "## Identify quality issues\n",
    "\n",
    "In the next step we want to identify quality issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d67642b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Quality Issues Found:\n"
     ]
    }
   ],
   "source": [
    "def identify_quality_issues(df):\n",
    "    \"\"\"Comprehensive data quality assessment\"\"\"\n",
    "    issues = {}\n",
    "\n",
    "    # Check for format inconsistencies\n",
    "    for col in df.select_dtypes(include=[\"object\"]).columns:\n",
    "        unique_patterns = df[col].astype(str).str.len().value_counts()\n",
    "        if len(unique_patterns) > 10:  # Many different lengths suggest format issues\n",
    "            issues[f\"{col}_format_inconsistency\"] = len(unique_patterns)\n",
    "\n",
    "    return issues\n",
    "\n",
    "\n",
    "# Run quality assessment\n",
    "quality_report = identify_quality_issues(df)\n",
    "print(\"Data Quality Issues Found:\")\n",
    "for issue, count in quality_report.items():\n",
    "    if count > 0:\n",
    "        print(f\"  {issue}: {count} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227a8058",
   "metadata": {},
   "source": [
    "## Statistical preprocessing pipeline\n",
    "\n",
    "Since we want to apply the same preprocessing steps to both the training and testing data, we will create a preprocessing pipeline. This pipeline will be fitted on the training data and then applied to both the training and testing data. The three steps in the pipeline are:\n",
    "\n",
    "1. **Imputation of missing values:** shouldn't do anything since we dropped missing values already - but to ensure a robust pipeline\n",
    "2. **Scaling of numerical features:** will make the model more performant\n",
    "3. **Select best features:** will help to reduce overfitting - while tuning the model I found out that we can use all features to a certain extent without overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8431dd9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                0\n",
       "yield_class       0\n",
       "age               0\n",
       "average_height    0\n",
       "dbh               0\n",
       "taper             0\n",
       "trees_per_ha      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CORRECT: Learn all parameters from training data only\n",
    "def create_preprocessing_pipeline(X_train, y_train):\n",
    "    \"\"\"Create preprocessing pipeline fitted on training data\"\"\"\n",
    "\n",
    "    # 1. Missing value imputation\n",
    "    imputer = SimpleImputer(strategy=\"mean\")\n",
    "    X_train_imputed = pd.DataFrame(\n",
    "        imputer.fit_transform(X_train),\n",
    "        columns=X_train.columns,  # restore column names\n",
    "        index=X_train.index,  # restore original index\n",
    "    )\n",
    "\n",
    "    # 2. Feature scaling\n",
    "    scaler = (\n",
    "        RobustScaler()\n",
    "    )  # Using RobustScaler instead of StandardScaler since it handles outliers\n",
    "    X_train_scaled = pd.DataFrame(\n",
    "        scaler.fit_transform(X_train_imputed),\n",
    "        columns=X_train_imputed.columns,\n",
    "        index=X_train_imputed.index,\n",
    "    )\n",
    "\n",
    "    # 3. Feature selection\n",
    "    selector = SelectKBest(f_regression, k=7)  # 7 in total\n",
    "    X_train_selected = pd.DataFrame(\n",
    "        selector.fit_transform(X_train_scaled, y_train),\n",
    "        columns=X_train_scaled.columns[selector.get_support()],\n",
    "        index=X_train_scaled.index,\n",
    "    )\n",
    "\n",
    "    # Return fitted preprocessors and transformed data\n",
    "    preprocessors = {\n",
    "        \"imputer\": imputer,\n",
    "        \"scaler\": scaler,\n",
    "        \"selector\": selector,\n",
    "    }\n",
    "\n",
    "    return X_train_selected, preprocessors\n",
    "\n",
    "\n",
    "def apply_preprocessing_pipeline(X_test, preprocessors):\n",
    "    \"\"\"Apply training preprocessing to test data\"\"\"\n",
    "\n",
    "    # Apply in same order as training\n",
    "    X_test_imputed = pd.DataFrame(\n",
    "        preprocessors[\"imputer\"].transform(X_test),\n",
    "        columns=X_test.columns,\n",
    "        index=X_test.index,\n",
    "    )\n",
    "    X_test_scaled = pd.DataFrame(\n",
    "        preprocessors[\"scaler\"].transform(X_test_imputed),\n",
    "        columns=X_test_imputed.columns,\n",
    "        index=X_test_imputed.index,\n",
    "    )\n",
    "    # Use selected columns from training\n",
    "    X_test_selected = pd.DataFrame(\n",
    "        preprocessors[\"selector\"].transform(X_test_scaled),\n",
    "        columns=X_test_scaled.columns[preprocessors[\"selector\"].get_support()],\n",
    "        index=X_test_scaled.index,\n",
    "    )\n",
    "\n",
    "    return X_test_selected\n",
    "\n",
    "\n",
    "# Usage\n",
    "X_train_processed, fitted_preprocessors = create_preprocessing_pipeline(\n",
    "    X_train, y_train\n",
    ")\n",
    "X_test_processed = apply_preprocessing_pipeline(X_test, fitted_preprocessors)\n",
    "\n",
    "# Print missing values\n",
    "X_train_processed.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86e3874",
   "metadata": {},
   "source": [
    "## Save the train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d39e0c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.to_csv(\"../data/processed/y_train.csv\", index=False)\n",
    "y_test.to_csv(\"../data/processed/y_test.csv\", index=False)\n",
    "X_train_processed.to_csv(\"../data/processed/X_train.csv\", index=False)\n",
    "X_test_processed.to_csv(\"../data/processed/X_test.csv\", index=False)\n",
    "X_train.to_csv(\"../data/processed/X_train_unprocessed.csv\", index=False)\n",
    "X_test.to_csv(\"../data/processed/X_test_unprocessed.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
