{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfbb708f",
   "metadata": {},
   "source": [
    "# Data cleanup\n",
    "\n",
    "In this notebook, we will perform data cleanup operations on the Agrofood CO2 emission dataset. This includes removing duplicates, handling missing values, and providing a summary of the dataset's features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b03f66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85729854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(\"../data/raw/yield_tables.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5eb1b8d",
   "metadata": {},
   "source": [
    "## Define relevant columns\n",
    "\n",
    "Columns relevant for building the regression model:\n",
    "\n",
    "- `id`: The unique identifier of the yield table - refers to the tree species.\n",
    "- `yield_class`: The yield class.\n",
    "- `age`: The age in years.\n",
    "- `average_height`: The average height in meters.\n",
    "- `dbh`: The diameter at breast height in centimeters.\n",
    "- `taper`: The tree taper.\n",
    "- `trees_per_ha`: The number of trees per hectare.\n",
    "- `volume_per_ha`: The volume per hectare in vfm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfa6e410",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_columns = [\n",
    "    \"id\",\n",
    "    \"yield_class\",\n",
    "    \"age\",\n",
    "    \"average_height\",\n",
    "    \"dbh\",\n",
    "    \"taper\",\n",
    "    \"trees_per_ha\",\n",
    "    \"volume_per_ha\",\n",
    "]\n",
    "df = df[relevant_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c256848f",
   "metadata": {},
   "source": [
    "## Remove duplicates\n",
    "\n",
    "Next, we will remove duplicate rows from the dataset and drop any rows with missing values. Just dropping the columns is not critical, since when looking at the dataset you see, that it's consistent per tree species. This means if there is data for one tree species (grouped by the `id` column) it's data for all entries of that species. Therefore, we can safely drop duplicates and missing values without losing important information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b48f6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact duplicates found: 5\n",
      "Total rows involved in duplication: 10\n",
      "\n",
      "\n",
      "Cleaning summary: {'original_rows': 14398, 'exact_duplicates_removed': 5, 'rows_with_missing_values_removed': 9974, 'final_rows': 4419}\n"
     ]
    }
   ],
   "source": [
    "# Check for exact duplicates across all columns\n",
    "exact_duplicates = df.duplicated().sum()\n",
    "print(f\"Exact duplicates found: {exact_duplicates}\")\n",
    "\n",
    "# Remove exact duplicates (keep first occurrence)\n",
    "df_duplicates_removed = df.drop_duplicates(keep=\"first\")\n",
    "df_cleaned = df_duplicates_removed.dropna(subset=relevant_columns)  # drop the NaN values\n",
    "\n",
    "\n",
    "# View duplicate rows\n",
    "duplicate_rows = df[df.duplicated(keep=False)]\n",
    "print(f\"Total rows involved in duplication: {len(duplicate_rows)}\")\n",
    "\n",
    "cleaning_log = {\n",
    "    \"original_rows\": len(df),\n",
    "    \"exact_duplicates_removed\": len(df) - len(df_duplicates_removed),\n",
    "    \"rows_with_missing_values_removed\": len(df_duplicates_removed) - len(df_cleaned),\n",
    "    \"final_rows\": len(df_cleaned),\n",
    "}\n",
    "print(f\"\\n\\nCleaning summary: {cleaning_log}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd1eb16",
   "metadata": {},
   "source": [
    "## Feature summary\n",
    "\n",
    "Last, we will show a summary of the features in the dataset after cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ccb1c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Feature summary table:                   Feature Name     Type  Missing?  Unique Values\n",
      "id                          id    int64       0.0             30\n",
      "yield_class        yield_class  float64       0.0             30\n",
      "age                        age    int64       0.0             30\n",
      "average_height  average_height  float64       0.0           1312\n",
      "dbh                        dbh  float64       0.0           1619\n",
      "taper                    taper  float64       0.0           1476\n",
      "trees_per_ha      trees_per_ha  float64       0.0           1826\n",
      "volume_per_ha    volume_per_ha  float64       0.0           1222\n"
     ]
    }
   ],
   "source": [
    "summary = pd.DataFrame(\n",
    "    {\n",
    "        \"Feature Name\": df_cleaned.columns,\n",
    "        \"Type\": df_cleaned.dtypes,\n",
    "        \"Missing?\": df_cleaned.isnull().mean().round(2),\n",
    "        \"Unique Values\": df_cleaned.nunique(),\n",
    "    }\n",
    ")\n",
    "print(f\"\\n\\nFeature summary table: {summary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6007f118",
   "metadata": {},
   "source": [
    "## Save the cleaned up data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eac0c64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.to_csv(\"../data/interim/cleaned_data.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
