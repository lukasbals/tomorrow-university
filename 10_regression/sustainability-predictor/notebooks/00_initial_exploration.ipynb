{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3e8f7ca",
   "metadata": {},
   "source": [
    "# Initial Exploration\n",
    "\n",
    "This notebook includes initial exploration of the dataset, for a structured data science project, see the following notebooks. If you want to get an initial idea of the dataset, check this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4249520e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectKBest, f_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce5141f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(\"../data/raw/Agrofood_co2_emission.csv\")\n",
    "\n",
    "relevant_columns = ['Year', 'Area', 'total_emission', 'Savanna fires', 'Forest fires']\n",
    "df = df[relevant_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63bc0bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Year         Area  total_emission  Savanna fires  Forest fires\n",
      "0  1990  Afghanistan     2198.963539        14.7237        0.0557\n",
      "1  1991  Afghanistan     2323.876629        14.7237        0.0557\n",
      "2  1992  Afghanistan     2356.304229        14.7237        0.0557\n",
      "3  1993  Afghanistan     2368.470529        14.7237        0.0557\n",
      "4  1994  Afghanistan     2500.768729        14.7237        0.0557\n",
      "Time range:  [1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003\n",
      " 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017\n",
      " 2018 2019 2020]\n"
     ]
    }
   ],
   "source": [
    "# Print data\n",
    "print(df.head())\n",
    "print(\"Time range: \", df['Year'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c33cda69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact duplicates found: 0\n",
      "Total rows involved in duplication: 0\n",
      "Duplicates based on Year and Area: 0\n",
      "Cleaning summary: {'original_rows': 6965, 'exact_duplicates_removed': 93, 'final_rows': 6872}\n",
      "Feature summary table:                    Feature Name     Type  Missing?  Unique Values\n",
      "Year                      Year    int64      0.00             31\n",
      "Area                      Area   object      0.00            236\n",
      "total_emission  total_emission  float64      0.00           6899\n",
      "Savanna fires    Savanna fires  float64      0.00           3746\n",
      "Forest fires      Forest fires  float64      0.01           2962\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicates\n",
    "\n",
    "# Check for exact duplicates across all columns\n",
    "exact_duplicates = df.duplicated().sum()\n",
    "print(f\"Exact duplicates found: {exact_duplicates}\")\n",
    "\n",
    "# View duplicate rows\n",
    "duplicate_rows = df[df.duplicated(keep=False)]\n",
    "print(f\"Total rows involved in duplication: {len(duplicate_rows)}\")\n",
    "\n",
    "# Check duplicates on specific key columns\n",
    "key_duplicates = df.duplicated(subset=['Year', 'Area']).sum()\n",
    "print(f\"Duplicates based on Year and Area: {key_duplicates}\")\n",
    "\n",
    "# Remove exact duplicates (keep first occurrence)\n",
    "df_cleaned = df.drop_duplicates(keep='first')\n",
    "df_cleaned = df_cleaned.dropna(subset=relevant_columns)\n",
    "\n",
    "cleaning_log = {\n",
    "    'original_rows': len(df),\n",
    "    'exact_duplicates_removed': len(df) - len(df_cleaned),\n",
    "    'final_rows': len(df_cleaned)\n",
    "}\n",
    "print(f\"Cleaning summary: {cleaning_log}\")\n",
    "\n",
    "summary = pd.DataFrame({\n",
    "    'Feature Name': df.columns,\n",
    "    'Type': df.dtypes,\n",
    "    'Missing?': df.isnull().mean().round(2),\n",
    "    'Unique Values': df.nunique()\n",
    "})\n",
    "print('Feature summary table: ', summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a78e2386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: title={'center': 'Total Emission Over Years'}, xlabel='Area'>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize features I want to use as variables\n",
    "\n",
    "df_cleaned.boxplot(figsize=(8, 4), column=[\"Forest fires\", \"Savanna fires\"])\n",
    "\n",
    "# Print data for Austria\n",
    "df_cleaned[df_cleaned['Area'] == \"Austria\"].plot(x='Year', y=['total_emission'], figsize=(4, 4), title='Total Emission Over Years')\n",
    "df_cleaned[df_cleaned['Area'] == \"Austria\"].plot(x='Year', y=['Forest fires', 'Savanna fires'], figsize=(4, 4), title='Total Emission Over Years')\n",
    "\n",
    "# Print data for Germany\n",
    "df_cleaned[df_cleaned['Area'] == \"Germany\"].plot(x='Year', y=['total_emission'], figsize=(4, 4), title='Total Emission Over Years')\n",
    "df_cleaned[df_cleaned['Area'] == \"Germany\"].plot(x='Year', y=['Forest fires', 'Savanna fires'], figsize=(4, 4), title='Total Emission Over Years')\n",
    "\n",
    "# Print data for Brazil\n",
    "df_cleaned[df_cleaned['Area'] == \"Brazil\"].plot(x='Year', y=['total_emission'], figsize=(4, 4), title='Total Emission Over Years')\n",
    "df_cleaned[df_cleaned['Area'] == \"Brazil\"].plot(x='Year', y=['Forest fires', 'Savanna fires'], figsize=(4, 4), title='Total Emission Over Years')\n",
    "\n",
    "df_cleaned['Area'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5050b37d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size:  5080\n",
      "Test data size:  1792\n",
      "Percent of total data (train):  73.9231664726426\n",
      "Percent of total data (test):  26.07683352735739\n"
     ]
    }
   ],
   "source": [
    "# Split data\n",
    "\n",
    "split_date = 2013\n",
    "train_data = df_cleaned[df_cleaned['Year'] < split_date]\n",
    "test_data = df_cleaned[df_cleaned['Year'] >= split_date]\n",
    "\n",
    "X_train = train_data.drop(columns=train_data.columns.difference(['Area', 'Savanna fires', 'Forest fires']))\n",
    "y_train = train_data['total_emission']\n",
    "X_test = test_data.drop(columns=test_data.columns.difference(['Area', 'Savanna fires', 'Forest fires']))\n",
    "y_test = test_data['total_emission']\n",
    "\n",
    "print(\"Training data size: \", train_data.shape[0])\n",
    "print(\"Test data size: \", test_data.shape[0])\n",
    "print(\"Percent of total data (train): \", train_data.shape[0] / df_cleaned.shape[0] * 100)\n",
    "print(\"Percent of total data (test): \", test_data.shape[0] / df_cleaned.shape[0] * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb446b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Quality Issues Found:\n",
      "  Area_format_inconsistency: 31 records\n"
     ]
    }
   ],
   "source": [
    "def identify_quality_issues(df):\n",
    "    \"\"\"Comprehensive data quality assessment\"\"\"\n",
    "    issues = {}\n",
    "    \n",
    "    # Check for impossible values (domain-specific)\n",
    "    if 'temperature' in df.columns:\n",
    "        impossible_temps = df[(df['temperature'] < -50) | (df['temperature'] > 70)]\n",
    "        issues['impossible_temperatures'] = len(impossible_temps)\n",
    "    \n",
    "    if 'energy_consumption' in df.columns:\n",
    "        negative_energy = df[df['energy_consumption'] < 0]\n",
    "        issues['negative_energy'] = len(negative_energy)\n",
    "    \n",
    "    # Check for future dates\n",
    "    if 'timestamp' in df.columns:\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "        future_dates = df[df['timestamp'] > datetime.now()]\n",
    "        issues['future_dates'] = len(future_dates)\n",
    "    \n",
    "    # Check for format inconsistencies\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        unique_patterns = df[col].astype(str).str.len().value_counts()\n",
    "        if len(unique_patterns) > 10:  # Many different lengths suggest format issues\n",
    "            issues[f'{col}_format_inconsistency'] = len(unique_patterns)\n",
    "    \n",
    "    return issues\n",
    "\n",
    "# Run quality assessment\n",
    "quality_report = identify_quality_issues(df_cleaned)\n",
    "print(\"Data Quality Issues Found:\")\n",
    "for issue, count in quality_report.items():\n",
    "    if count > 0:\n",
    "        print(f\"  {issue}: {count} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2327a53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_one_hot_encoding(X_train, categorical_features):\n",
    "    \"\"\"Apply one-hot encoding to categorical features\"\"\"\n",
    "    \n",
    "    encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "    \n",
    "    # Fit encoder on training data\n",
    "    encoded_features = encoder.fit_transform(X_train[categorical_features])\n",
    "    \n",
    "    # Get feature names\n",
    "    feature_names = encoder.get_feature_names_out(categorical_features)\n",
    "    \n",
    "    # Create DataFrame with encoded features\n",
    "    encoded_df = pd.DataFrame(encoded_features, columns=feature_names, index=X_train.index)\n",
    "    \n",
    "    return encoded_df, encoder\n",
    "\n",
    "# Example usage\n",
    "categorical_features = ['Area']\n",
    "X_train_encoded, fitted_encoder = apply_one_hot_encoding(X_train, categorical_features)\n",
    "X_test_encoded = fitted_encoder.transform(X_test[categorical_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f186c4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores (negative MSE): [-5.16280340e+09 -4.15112391e+09 -4.09339382e+09 -3.30658571e+09\n",
      " -4.92695693e+09]\n"
     ]
    }
   ],
   "source": [
    "# Cross validate\n",
    "\n",
    "# Basic K-Fold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Evaluate model performance\n",
    "model = LinearRegression()\n",
    "cv_scores = cross_val_score(model, X_train_encoded, y_train, cv=kfold, \n",
    "                           scoring='neg_mean_squared_error')\n",
    "\n",
    "print(\"Cross-validation scores (negative MSE):\", cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "53eb8415",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lukasbals/tomorrow-university/10_regression/sustainability-predictor/.venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but SimpleImputer was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Statistical preprocessing pipeline\n",
    "\n",
    "# CORRECT: Learn all parameters from training data only\n",
    "def create_preprocessing_pipeline(X_train, y_train):\n",
    "    \"\"\"Create preprocessing pipeline fitted on training data\"\"\"\n",
    "    \n",
    "    # 1. Missing value imputation\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    X_train_imputed = imputer.fit_transform(X_train)\n",
    "    \n",
    "    # 2. Feature scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "    \n",
    "    # 3. Feature selection\n",
    "    selector = SelectKBest(f_regression, k=10)\n",
    "    X_train_selected = selector.fit_transform(X_train_scaled, y_train)\n",
    "    \n",
    "    # Return fitted preprocessors and transformed data\n",
    "    preprocessors = {\n",
    "        'imputer': imputer,\n",
    "        'scaler': scaler,\n",
    "        'selector': selector\n",
    "    }\n",
    "    \n",
    "    return X_train_selected, preprocessors\n",
    "\n",
    "def apply_preprocessing_pipeline(X_test, preprocessors):\n",
    "    \"\"\"Apply training preprocessing to test data\"\"\"\n",
    "    \n",
    "    # Apply in same order as training\n",
    "    X_test_imputed = preprocessors['imputer'].transform(X_test)\n",
    "    X_test_scaled = preprocessors['scaler'].transform(X_test_imputed)\n",
    "    X_test_selected = preprocessors['selector'].transform(X_test_scaled)\n",
    "    \n",
    "    return X_test_selected\n",
    "\n",
    "# Usage\n",
    "X_train_processed, fitted_preprocessors = create_preprocessing_pipeline(X_train_encoded, y_train)\n",
    "X_test_processed = apply_preprocessing_pipeline(X_test_encoded, fitted_preprocessors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
