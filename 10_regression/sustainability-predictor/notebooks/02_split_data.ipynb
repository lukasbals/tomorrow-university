{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f68a806",
   "metadata": {},
   "source": [
    "# Split data\n",
    "\n",
    "In this notebook, we split the data into training and testing sets. The result will be four datasets: `X_train`, `X_test`, `y_train`, and `y_test`, which will be stored in `../data/interim/`. Those datasets will be used for training and evaluating the machine learning model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "e4f5499e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import RobustScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "# Load the cleaned up data\n",
    "df = pd.read_csv(\"../data/interim/cleaned_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b835a37",
   "metadata": {},
   "source": [
    "## Handle categorical features\n",
    "\n",
    "Since we have one categorical feature ('Area'), we need to apply one-hot encoding to it. This makes sure, the categories are translated to numeric values that can be used by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "4e8dafb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have no categorical features to encode\n",
    "\n",
    "# label_encoder = LabelEncoder()\n",
    "# df.Area = label_encoder.fit_transform(df.Area)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8923d959",
   "metadata": {},
   "source": [
    "## How to split the data\n",
    "\n",
    "Target variable: `Average Temperature Â°C`\n",
    "Features: All other features of the dataset\n",
    "\n",
    "Categorical data on the dataset: `Area`\n",
    "\n",
    "We will use the year to do a temporal split since we want to predict values in the future. This means we will use data from the past to predict data from the future.\n",
    "\n",
    "We will split between the year 2012 and 2013 since then we have a good amount of data for training and can use the subsequent years for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "a94721a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size:  8000\n",
      "Test data size:  2000\n",
      "Percent of total data (train):  80.0\n",
      "Percent of total data (test):  20.0\n"
     ]
    }
   ],
   "source": [
    "feature_columns = df.columns.drop([\"price\"])\n",
    "X = df.drop(\"price\", axis=1)\n",
    "y = df[\"price\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,   # 20% for test\n",
    "    random_state=42, # for reproducibility\n",
    ")\n",
    "\n",
    "print(\"Training data size: \", X_train.shape[0])\n",
    "print(\"Test data size: \", X_test.shape[0])\n",
    "print(\"Percent of total data (train): \", X_train.shape[0] / df.shape[0] * 100)\n",
    "print(\"Percent of total data (test): \", X_test.shape[0] / df.shape[0] * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382d033a",
   "metadata": {},
   "source": [
    "## Identify quality issues\n",
    "\n",
    "In the next step we want to identify quality issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "d67642b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Quality Issues Found:\n"
     ]
    }
   ],
   "source": [
    "def identify_quality_issues(df):\n",
    "    \"\"\"Comprehensive data quality assessment\"\"\"\n",
    "    issues = {}\n",
    "\n",
    "    if \"Forest fires\" in df.columns:\n",
    "        negative_fires = df[df[\"Forest fires\"] < 0]\n",
    "        issues[\"negative_fires\"] = len(negative_fires)\n",
    "    if \"Savanna fires\" in df.columns:\n",
    "        negative_savanna = df[df[\"Savanna fires\"] < 0]\n",
    "        issues[\"negative_savanna\"] = len(negative_savanna)\n",
    "\n",
    "    # Check for future dates\n",
    "    if \"Year\" in df.columns:\n",
    "        df[\"Year\"] = pd.to_datetime(df[\"Year\"], errors=\"coerce\")\n",
    "        current_year = datetime.datetime.now().year\n",
    "        future_dates = df[df[\"Year\"].dt.year > current_year]\n",
    "        issues[\"future_dates\"] = len(future_dates)\n",
    "\n",
    "    # Check for format inconsistencies\n",
    "    for col in df.select_dtypes(include=[\"object\"]).columns:\n",
    "        unique_patterns = df[col].astype(str).str.len().value_counts()\n",
    "        if len(unique_patterns) > 10:  # Many different lengths suggest format issues\n",
    "            issues[f\"{col}_format_inconsistency\"] = len(unique_patterns)\n",
    "\n",
    "    return issues\n",
    "\n",
    "\n",
    "# Run quality assessment\n",
    "quality_report = identify_quality_issues(df)\n",
    "print(\"Data Quality Issues Found:\")\n",
    "for issue, count in quality_report.items():\n",
    "    if count > 0:\n",
    "        print(f\"  {issue}: {count} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227a8058",
   "metadata": {},
   "source": [
    "## Statistical preprocessing pipeline\n",
    "\n",
    "Since we want to apply the same preprocessing steps to both the training and testing data, we will create a preprocessing pipeline. This pipeline will be fitted on the training data and then applied to both the training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "8431dd9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "squareMeters         0\n",
       "numberOfRooms        0\n",
       "hasYard              0\n",
       "hasPool              0\n",
       "floors               0\n",
       "cityCode             0\n",
       "cityPartRange        0\n",
       "numPrevOwners        0\n",
       "made                 0\n",
       "isNewBuilt           0\n",
       "hasStormProtector    0\n",
       "basement             0\n",
       "attic                0\n",
       "garage               0\n",
       "hasStorageRoom       0\n",
       "hasGuestRoom         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CORRECT: Learn all parameters from training data only\n",
    "def create_preprocessing_pipeline(X_train, y_train):\n",
    "    \"\"\"Create preprocessing pipeline fitted on training data\"\"\"\n",
    "\n",
    "    # 1. Missing value imputation\n",
    "    imputer = SimpleImputer(strategy=\"mean\")\n",
    "    X_train_imputed = pd.DataFrame(\n",
    "        imputer.fit_transform(X_train),\n",
    "        columns=X_train.columns,  # restore column names\n",
    "        index=X_train.index,  # restore original index\n",
    "    )\n",
    "\n",
    "    # 2. Feature scaling\n",
    "    scaler = (\n",
    "        RobustScaler()\n",
    "    )  # Using RobustScaler instead of StandardScaler since it handles outliers\n",
    "    X_train_scaled = pd.DataFrame(\n",
    "        scaler.fit_transform(X_train_imputed),\n",
    "        columns=X_train_imputed.columns,\n",
    "        index=X_train_imputed.index,\n",
    "    )\n",
    "\n",
    "    # 3. Feature selection\n",
    "    selector = SelectKBest(\n",
    "        f_regression, k=16\n",
    "    )  # 31 total features including target variable\n",
    "    X_train_selected = pd.DataFrame(\n",
    "        selector.fit_transform(X_train_scaled, y_train),\n",
    "        columns=X_train_scaled.columns[selector.get_support()],\n",
    "        index=X_train_scaled.index,\n",
    "    )\n",
    "\n",
    "    # Return fitted preprocessors and transformed data\n",
    "    preprocessors = {\n",
    "        \"imputer\": imputer,\n",
    "        \"scaler\": scaler,\n",
    "        \"selector\": selector,\n",
    "    }\n",
    "\n",
    "    return X_train_selected, preprocessors\n",
    "\n",
    "\n",
    "def apply_preprocessing_pipeline(X_test, preprocessors):\n",
    "    \"\"\"Apply training preprocessing to test data\"\"\"\n",
    "\n",
    "    # Apply in same order as training\n",
    "    X_test_imputed = pd.DataFrame(\n",
    "        preprocessors[\"imputer\"].transform(X_test),\n",
    "        columns=X_test.columns,\n",
    "        index=X_test.index,\n",
    "    )\n",
    "    X_test_scaled = pd.DataFrame(\n",
    "        preprocessors[\"scaler\"].transform(X_test_imputed),\n",
    "        columns=X_test_imputed.columns,\n",
    "        index=X_test_imputed.index,\n",
    "    )\n",
    "    # Use selected columns from training\n",
    "    X_test_selected = pd.DataFrame(\n",
    "        preprocessors[\"selector\"].transform(X_test_scaled),\n",
    "        columns=X_test_scaled.columns[preprocessors[\"selector\"].get_support()],\n",
    "        index=X_test_scaled.index,\n",
    "    )\n",
    "\n",
    "    return X_test_selected\n",
    "\n",
    "\n",
    "# Usage\n",
    "X_train_processed, fitted_preprocessors = create_preprocessing_pipeline(\n",
    "    X_train, y_train\n",
    ")\n",
    "X_test_processed = apply_preprocessing_pipeline(X_test, fitted_preprocessors)\n",
    "\n",
    "# Print missing values\n",
    "X_train_processed.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86e3874",
   "metadata": {},
   "source": [
    "## Save the train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "d39e0c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.to_csv(\"../data/processed/y_train.csv\", index=False)\n",
    "y_test.to_csv(\"../data/processed/y_test.csv\", index=False)\n",
    "X_train_processed.to_csv(\"../data/processed/X_train.csv\", index=False)\n",
    "X_test_processed.to_csv(\"../data/processed/X_test.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
