{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f68a806",
   "metadata": {},
   "source": [
    "# Split data\n",
    "\n",
    "In this notebook, we split the data into training and testing sets. The result will be four datasets: `X_train`, `X_test`, `y_train`, and `y_test`, which will be stored in `../data/interim/`. Those datasets will be used for training and evaluating the machine learning model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e4f5499e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.pipeline import Pipeline\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "# Load the cleaned up data\n",
    "df = pd.read_csv('../data/interim/cleaned_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8923d959",
   "metadata": {},
   "source": [
    "## How to split the data\n",
    "\n",
    "Target variable: `total_emission`\n",
    "Features: `Savanna fires`, `Forest fires`\n",
    "\n",
    "Other data on the dataset: `Year`, `Area`\n",
    "\n",
    "We will use the year to do a temporal split since we want to predict values in the future. This means we will use data from the past to predict data from the future.\n",
    "\n",
    "We will split between the year 2012 and 2013 since then we have a good amount of data for training and can use the subsequent years for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a94721a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size:  5080\n",
      "Test data size:  1792\n",
      "Percent of total data (train):  73.9231664726426\n",
      "Percent of total data (test):  26.07683352735739\n"
     ]
    }
   ],
   "source": [
    "\n",
    "split_year = 2013\n",
    "train_data = df[df['Year'] < split_year]\n",
    "test_data = df[df['Year'] >= split_year]\n",
    "\n",
    "X_train = train_data.drop(columns=train_data.columns.difference(['Year', 'Area', 'Savanna fires', 'Forest fires']))\n",
    "y_train = train_data['total_emission']\n",
    "X_test = test_data.drop(columns=test_data.columns.difference(['Year', 'Area', 'Savanna fires', 'Forest fires']))\n",
    "y_test = test_data['total_emission']\n",
    "\n",
    "print(\"Training data size: \", train_data.shape[0])\n",
    "print(\"Test data size: \", test_data.shape[0])\n",
    "print(\"Percent of total data (train): \", train_data.shape[0] / df.shape[0] * 100)\n",
    "print(\"Percent of total data (test): \", test_data.shape[0] / df.shape[0] * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382d033a",
   "metadata": {},
   "source": [
    "## Identify quality issues\n",
    "\n",
    "In the next step we want to identify quality issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d67642b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Quality Issues Found:\n",
      "  Area_format_inconsistency: 31 records\n"
     ]
    }
   ],
   "source": [
    "def identify_quality_issues(df):\n",
    "    \"\"\"Comprehensive data quality assessment\"\"\"\n",
    "    issues = {}\n",
    "\n",
    "    if 'Forest fires' in df.columns:\n",
    "        negative_fires = df[df['Forest fires'] < 0]\n",
    "        issues['negative_fires'] = len(negative_fires)\n",
    "    if 'Savanna fires' in df.columns:\n",
    "        negative_savanna = df[df['Savanna fires'] < 0]\n",
    "        issues['negative_savanna'] = len(negative_savanna)\n",
    "\n",
    "    # Check for future dates\n",
    "    if 'Year' in df.columns:\n",
    "        df['Year'] = pd.to_datetime(df['Year'], errors='coerce')\n",
    "        current_year = datetime.datetime.now().year\n",
    "        future_dates = df[df['Year'].dt.year > current_year]\n",
    "        issues['future_dates'] = len(future_dates)\n",
    "    \n",
    "    # Check for format inconsistencies\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        unique_patterns = df[col].astype(str).str.len().value_counts()\n",
    "        if len(unique_patterns) > 10:  # Many different lengths suggest format issues\n",
    "            issues[f'{col}_format_inconsistency'] = len(unique_patterns)\n",
    "    \n",
    "    return issues\n",
    "\n",
    "# Run quality assessment\n",
    "quality_report = identify_quality_issues(df)\n",
    "print(\"Data Quality Issues Found:\")\n",
    "for issue, count in quality_report.items():\n",
    "    if count > 0:\n",
    "        print(f\"  {issue}: {count} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa547a33",
   "metadata": {},
   "source": [
    "## Handle categorical features\n",
    "\n",
    "Since we have one categorical feature ('Area'), we need to apply one-hot encoding to it. This makes sure, the categories are translated to numeric values that can be used by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "355be7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_one_hot_encoding(X_train, categorical_features):\n",
    "    \"\"\"Apply one-hot encoding to categorical features\"\"\"\n",
    "    \n",
    "    encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "    \n",
    "    # Fit encoder on training data\n",
    "    encoded_features = encoder.fit_transform(X_train[categorical_features])\n",
    "    \n",
    "    # Get feature names\n",
    "    feature_names = encoder.get_feature_names_out(categorical_features)\n",
    "    \n",
    "    # Create DataFrame with encoded features\n",
    "    encoded_df = pd.DataFrame(encoded_features, columns=feature_names, index=X_train.index)\n",
    "    \n",
    "    return encoded_df, encoder\n",
    "\n",
    "# Example usage\n",
    "categorical_features = ['Area']\n",
    "\n",
    "X_train_encoded, fitted_encoder = apply_one_hot_encoding(X_train, categorical_features)\n",
    "X_test_encoded = fitted_encoder.transform(X_test[categorical_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227a8058",
   "metadata": {},
   "source": [
    "## Statistical preprocessing pipeline\n",
    "\n",
    "Since we want to apply the same preprocessing steps to both the training and testing data, we will create a preprocessing pipeline. This pipeline will be fitted on the training data and then applied to both the training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8431dd9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lukasbals/tomorrow-university/10_regression/sustainability-predictor/.venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but SimpleImputer was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# CORRECT: Learn all parameters from training data only\n",
    "def create_preprocessing_pipeline(X_train, y_train):\n",
    "    \"\"\"Create preprocessing pipeline fitted on training data\"\"\"\n",
    "    \n",
    "    # 1. Missing value imputation\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    X_train_imputed = imputer.fit_transform(X_train)\n",
    "    \n",
    "    # 2. Feature scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "    \n",
    "    # 3. Feature selection\n",
    "    selector = SelectKBest(f_regression, k=10)\n",
    "    X_train_selected = selector.fit_transform(X_train_scaled, y_train)\n",
    "    \n",
    "    # Return fitted preprocessors and transformed data\n",
    "    preprocessors = {\n",
    "        'imputer': imputer,\n",
    "        'scaler': scaler,\n",
    "        'selector': selector\n",
    "    }\n",
    "    \n",
    "    return X_train_selected, preprocessors\n",
    "\n",
    "def apply_preprocessing_pipeline(X_test, preprocessors):\n",
    "    \"\"\"Apply training preprocessing to test data\"\"\"\n",
    "    \n",
    "    # Apply in same order as training\n",
    "    X_test_imputed = preprocessors['imputer'].transform(X_test)\n",
    "    X_test_scaled = preprocessors['scaler'].transform(X_test_imputed)\n",
    "    X_test_selected = preprocessors['selector'].transform(X_test_scaled)\n",
    "    \n",
    "    return X_test_selected\n",
    "\n",
    "# Usage\n",
    "X_train_processed, fitted_preprocessors = create_preprocessing_pipeline(X_train_encoded, y_train)\n",
    "X_test_processed = apply_preprocessing_pipeline(X_test_encoded, fitted_preprocessors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86e3874",
   "metadata": {},
   "source": [
    "## Save the train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d39e0c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.to_csv(\"../data/processed/y_train.csv\", index=False)\n",
    "y_test.to_csv(\"../data/processed/y_test.csv\", index=False)\n",
    "np.savetxt(\"../data/processed/X_train.csv\", X_train_processed, delimiter=\",\")\n",
    "np.savetxt(\"../data/processed/X_test.csv\", X_test_processed, delimiter=\",\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
