{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f68a806",
   "metadata": {},
   "source": [
    "# Split data\n",
    "\n",
    "In this notebook, we split the data into training and testing sets. The result will be four datasets: `X_train`, `X_test`, `y_train`, and `y_test`, which will be stored in `../data/interim/`. Those datasets will be used for training and evaluating the machine learning model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "e4f5499e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "# Load the cleaned up data\n",
    "df = pd.read_csv('../data/interim/cleaned_data.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b835a37",
   "metadata": {},
   "source": [
    "## Handle categorical features\n",
    "\n",
    "Since we have one categorical feature ('Area'), we need to apply one-hot encoding to it. This makes sure, the categories are translated to numeric values that can be used by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "4e8dafb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "df.Area = label_encoder.fit_transform(df.Area)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8923d959",
   "metadata": {},
   "source": [
    "## How to split the data\n",
    "\n",
    "Target variable: `Average Temperature 째C`\n",
    "Features: All other features of the dataset\n",
    "\n",
    "Categorical data on the dataset: `Area`\n",
    "\n",
    "We will use the year to do a temporal split since we want to predict values in the future. This means we will use data from the past to predict data from the future.\n",
    "\n",
    "We will split between the year 2012 and 2013 since then we have a good amount of data for training and can use the subsequent years for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "a94721a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size:  4469\n",
      "Test data size:  2496\n",
      "Percent of total data (train):  64.16367552045944\n",
      "Percent of total data (test):  35.83632447954056\n"
     ]
    }
   ],
   "source": [
    "split_year = 2010\n",
    "feature_columns = df.columns.drop(['Average Temperature 째C'])\n",
    "\n",
    "train_data = df[df['Year'] < split_year]\n",
    "test_data = df[df['Year'] >= split_year]\n",
    "\n",
    "X_train = train_data[feature_columns]\n",
    "y_train = train_data['Average Temperature 째C']\n",
    "X_test = test_data[feature_columns]\n",
    "y_test = test_data['Average Temperature 째C']\n",
    "\n",
    "print(\"Training data size: \", train_data.shape[0])\n",
    "print(\"Test data size: \", test_data.shape[0])\n",
    "print(\"Percent of total data (train): \", train_data.shape[0] / df.shape[0] * 100)\n",
    "print(\"Percent of total data (test): \", test_data.shape[0] / df.shape[0] * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382d033a",
   "metadata": {},
   "source": [
    "## Identify quality issues\n",
    "\n",
    "In the next step we want to identify quality issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "d67642b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Quality Issues Found:\n"
     ]
    }
   ],
   "source": [
    "def identify_quality_issues(df):\n",
    "    \"\"\"Comprehensive data quality assessment\"\"\"\n",
    "    issues = {}\n",
    "\n",
    "    if 'Forest fires' in df.columns:\n",
    "        negative_fires = df[df['Forest fires'] < 0]\n",
    "        issues['negative_fires'] = len(negative_fires)\n",
    "    if 'Savanna fires' in df.columns:\n",
    "        negative_savanna = df[df['Savanna fires'] < 0]\n",
    "        issues['negative_savanna'] = len(negative_savanna)\n",
    "\n",
    "    # Check for future dates\n",
    "    if 'Year' in df.columns:\n",
    "        df['Year'] = pd.to_datetime(df['Year'], errors='coerce')\n",
    "        current_year = datetime.datetime.now().year\n",
    "        future_dates = df[df['Year'].dt.year > current_year]\n",
    "        issues['future_dates'] = len(future_dates)\n",
    "    \n",
    "    # Check for format inconsistencies\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        unique_patterns = df[col].astype(str).str.len().value_counts()\n",
    "        if len(unique_patterns) > 10:  # Many different lengths suggest format issues\n",
    "            issues[f'{col}_format_inconsistency'] = len(unique_patterns)\n",
    "    \n",
    "    return issues\n",
    "\n",
    "# Run quality assessment\n",
    "quality_report = identify_quality_issues(df)\n",
    "print(\"Data Quality Issues Found:\")\n",
    "for issue, count in quality_report.items():\n",
    "    if count > 0:\n",
    "        print(f\"  {issue}: {count} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943bde86",
   "metadata": {},
   "source": [
    "## Handle categorical features\n",
    "\n",
    "Since we have one categorical feature ('Area'), we need to apply one-hot encoding to it. This makes sure, the categories are translated to numeric values that can be used by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "355be7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def apply_one_hot_encoding(X_train, categorical_features):\n",
    "#     \"\"\"Apply one-hot encoding to categorical features\"\"\"\n",
    "    \n",
    "#     encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "    \n",
    "#     # Fit encoder on training data\n",
    "#     encoded_features = encoder.fit_transform(X_train[categorical_features])\n",
    "    \n",
    "#     # Get feature names\n",
    "#     feature_names = encoder.get_feature_names_out(categorical_features)\n",
    "    \n",
    "#     # Create DataFrame with encoded features\n",
    "#     encoded_df = pd.DataFrame(encoded_features, columns=feature_names, index=X_train.index)\n",
    "    \n",
    "#     return encoded_df, encoder\n",
    "\n",
    "# # Example usage\n",
    "# categorical_features = ['Area']\n",
    "\n",
    "# X_train_encoded, fitted_encoder = apply_one_hot_encoding(X_train, categorical_features)\n",
    "# X_test_encoded = fitted_encoder.transform(X_test[categorical_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227a8058",
   "metadata": {},
   "source": [
    "## Statistical preprocessing pipeline\n",
    "\n",
    "Since we want to apply the same preprocessing steps to both the training and testing data, we will create a preprocessing pipeline. This pipeline will be fitted on the training data and then applied to both the training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "8431dd9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Year                               0\n",
       "Savanna fires                      0\n",
       "Forest fires                       0\n",
       "Drained organic soils (CO2)        0\n",
       "Pesticides Manufacturing           0\n",
       "Food Transport                     0\n",
       "Forestland                         0\n",
       "Food Household Consumption         0\n",
       "Food Retail                        0\n",
       "Food Packaging                     0\n",
       "Food Processing                    0\n",
       "Fertilizers Manufacturing          0\n",
       "IPPU                               0\n",
       "Manure applied to Soils            0\n",
       "Manure Management                  0\n",
       "Fires in humid tropical forests    0\n",
       "On-farm energy use                 0\n",
       "Urban population                   0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CORRECT: Learn all parameters from training data only\n",
    "def create_preprocessing_pipeline(X_train, y_train):\n",
    "    \"\"\"Create preprocessing pipeline fitted on training data\"\"\"\n",
    "    \n",
    "    # 1. Missing value imputation\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    X_train_imputed = pd.DataFrame(\n",
    "        imputer.fit_transform(X_train),\n",
    "        columns=X_train.columns,    # restore column names\n",
    "        index=X_train.index         # restore original index\n",
    "    )\n",
    "    \n",
    "    # 2. Feature scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = pd.DataFrame(\n",
    "        scaler.fit_transform(X_train_imputed),\n",
    "        columns=X_train_imputed.columns,\n",
    "        index=X_train_imputed.index\n",
    "    )\n",
    "\n",
    "    # 3. Feature selection\n",
    "    selector = SelectKBest(f_regression, k=18) # 31 total features including target variable\n",
    "    X_train_selected = pd.DataFrame(\n",
    "        selector.fit_transform(X_train_scaled, y_train),\n",
    "        columns=X_train_scaled.columns[selector.get_support()],\n",
    "        index=X_train_scaled.index\n",
    "    )\n",
    "\n",
    "    # Return fitted preprocessors and transformed data\n",
    "    preprocessors = {\n",
    "        'imputer': imputer,\n",
    "        'scaler': scaler,\n",
    "        'selector': selector\n",
    "    }\n",
    "\n",
    "    return X_train_selected, preprocessors\n",
    "\n",
    "def apply_preprocessing_pipeline(X_test, preprocessors):\n",
    "    \"\"\"Apply training preprocessing to test data\"\"\"\n",
    "    \n",
    "    # Apply in same order as training\n",
    "    X_test_imputed = pd.DataFrame(\n",
    "        preprocessors['imputer'].transform(X_test),\n",
    "        columns=X_test.columns,\n",
    "        index=X_test.index\n",
    "    )\n",
    "    X_test_scaled = pd.DataFrame(\n",
    "        preprocessors['scaler'].transform(X_test_imputed),\n",
    "        columns=X_test_imputed.columns,\n",
    "        index=X_test_imputed.index\n",
    "    )\n",
    "    X_test_selected = pd.DataFrame(\n",
    "        preprocessors['selector'].transform(X_test_scaled),\n",
    "        columns=X_test_scaled.columns[preprocessors['selector'].get_support()],\n",
    "        index=X_test_scaled.index\n",
    "    )\n",
    "\n",
    "    return X_test_selected\n",
    "\n",
    "# Usage\n",
    "X_train_processed, fitted_preprocessors = create_preprocessing_pipeline(X_train, y_train)\n",
    "X_test_processed = apply_preprocessing_pipeline(X_test, fitted_preprocessors)\n",
    "\n",
    "# Print missing values\n",
    "X_train_processed.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86e3874",
   "metadata": {},
   "source": [
    "## Save the train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "d39e0c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.to_csv(\"../data/processed/y_train.csv\", index=False)\n",
    "y_test.to_csv(\"../data/processed/y_test.csv\", index=False)\n",
    "X_train_processed.to_csv(\"../data/processed/X_train.csv\", index=False)\n",
    "X_test_processed.to_csv(\"../data/processed/X_test.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
