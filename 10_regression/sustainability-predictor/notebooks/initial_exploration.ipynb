{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3e8f7ca",
   "metadata": {},
   "source": [
    "# Initial Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "4249520e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "ce5141f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(\"../data/raw/Agrofood_co2_emission.csv\")\n",
    "\n",
    "relevant_columns = ['Year', 'Area', 'total_emission', 'Savanna fires', 'Forest fires']\n",
    "df = df[relevant_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "63bc0bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Year         Area  total_emission  Savanna fires  Forest fires\n",
      "0  1990  Afghanistan     2198.963539        14.7237        0.0557\n",
      "1  1991  Afghanistan     2323.876629        14.7237        0.0557\n",
      "2  1992  Afghanistan     2356.304229        14.7237        0.0557\n",
      "3  1993  Afghanistan     2368.470529        14.7237        0.0557\n",
      "4  1994  Afghanistan     2500.768729        14.7237        0.0557\n",
      "Time range:  [1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003\n",
      " 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017\n",
      " 2018 2019 2020]\n"
     ]
    }
   ],
   "source": [
    "# Print data\n",
    "print(df.head())\n",
    "print(\"Time range: \", df['Year'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "c33cda69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact duplicates found: 0\n",
      "Total rows involved in duplication: 0\n",
      "Duplicates based on Year and Area: 0\n",
      "Cleaning summary: {'original_rows': 6965, 'exact_duplicates_removed': 93, 'final_rows': 6872}\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicates\n",
    "\n",
    "# Check for exact duplicates across all columns\n",
    "exact_duplicates = df.duplicated().sum()\n",
    "print(f\"Exact duplicates found: {exact_duplicates}\")\n",
    "\n",
    "# View duplicate rows\n",
    "duplicate_rows = df[df.duplicated(keep=False)]\n",
    "print(f\"Total rows involved in duplication: {len(duplicate_rows)}\")\n",
    "\n",
    "# Check duplicates on specific key columns\n",
    "key_duplicates = df.duplicated(subset=['Year', 'Area']).sum()\n",
    "print(f\"Duplicates based on Year and Area: {key_duplicates}\")\n",
    "\n",
    "# Remove exact duplicates (keep first occurrence)\n",
    "df_cleaned = df.drop_duplicates(keep='first')\n",
    "df_cleaned = df_cleaned.dropna(subset=relevant_columns)\n",
    "\n",
    "cleaning_log = {\n",
    "    'original_rows': len(df),\n",
    "    'exact_duplicates_removed': len(df) - len(df_cleaned),\n",
    "    'final_rows': len(df_cleaned)\n",
    "}\n",
    "print(f\"Cleaning summary: {cleaning_log}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "bb446b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Quality Issues Found:\n",
      "  Area_format_inconsistency: 31 records\n"
     ]
    }
   ],
   "source": [
    "def identify_quality_issues(df):\n",
    "    \"\"\"Comprehensive data quality assessment\"\"\"\n",
    "    issues = {}\n",
    "    \n",
    "    # Check for impossible values (domain-specific)\n",
    "    if 'temperature' in df.columns:\n",
    "        impossible_temps = df[(df['temperature'] < -50) | (df['temperature'] > 70)]\n",
    "        issues['impossible_temperatures'] = len(impossible_temps)\n",
    "    \n",
    "    if 'energy_consumption' in df.columns:\n",
    "        negative_energy = df[df['energy_consumption'] < 0]\n",
    "        issues['negative_energy'] = len(negative_energy)\n",
    "    \n",
    "    # Check for future dates\n",
    "    if 'timestamp' in df.columns:\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "        future_dates = df[df['timestamp'] > datetime.now()]\n",
    "        issues['future_dates'] = len(future_dates)\n",
    "    \n",
    "    # Check for format inconsistencies\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        unique_patterns = df[col].astype(str).str.len().value_counts()\n",
    "        if len(unique_patterns) > 10:  # Many different lengths suggest format issues\n",
    "            issues[f'{col}_format_inconsistency'] = len(unique_patterns)\n",
    "    \n",
    "    return issues\n",
    "\n",
    "# Run quality assessment\n",
    "quality_report = identify_quality_issues(df_cleaned)\n",
    "print(\"Data Quality Issues Found:\")\n",
    "for issue, count in quality_report.items():\n",
    "    if count > 0:\n",
    "        print(f\"  {issue}: {count} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "5050b37d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size:  5080\n",
      "Test data size:  1792\n",
      "Percent of total data (train):  73.9231664726426\n",
      "Percent of total data (test):  26.07683352735739\n"
     ]
    }
   ],
   "source": [
    "# Split data\n",
    "\n",
    "split_date = 2013\n",
    "train_data = df_cleaned[df_cleaned['Year'] < split_date]\n",
    "test_data = df_cleaned[df_cleaned['Year'] >= split_date]\n",
    "\n",
    "X_train = train_data.drop(columns=train_data.columns.difference(['Savanna fires', 'Forest fires']))\n",
    "y_train = train_data['total_emission']\n",
    "X_test = test_data.drop(columns=test_data.columns.difference(['Savanna fires', 'Forest fires']))\n",
    "y_test = test_data['total_emission']\n",
    "\n",
    "print(\"Training data size: \", train_data.shape[0])\n",
    "print(\"Test data size: \", test_data.shape[0])\n",
    "print(\"Percent of total data (train): \", train_data.shape[0] / df_cleaned.shape[0] * 100)\n",
    "print(\"Percent of total data (test): \", test_data.shape[0] / df_cleaned.shape[0] * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "f186c4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores (negative MSE): [-4.59605240e+10 -3.18091324e+10 -3.88250730e+10 -3.73666600e+10\n",
      " -3.45606866e+10]\n"
     ]
    }
   ],
   "source": [
    "# Cross validate\n",
    "\n",
    "# Basic K-Fold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Evaluate model performance\n",
    "model = LinearRegression()\n",
    "cv_scores = cross_val_score(model, X_train, y_train, cv=kfold, \n",
    "                           scoring='neg_mean_squared_error')\n",
    "\n",
    "print(\"Cross-validation scores (negative MSE):\", cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53eb8415",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lukasbals/tomorrow-university/10_regression/sustainability-predictor/.venv/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:782: UserWarning: k=10 is greater than n_features=2. All the features will be returned.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Statistical preprocessing pipeline\n",
    "\n",
    "# CORRECT: Learn all parameters from training data only\n",
    "def create_preprocessing_pipeline(X_train, y_train):\n",
    "    \"\"\"Create preprocessing pipeline fitted on training data\"\"\"\n",
    "    \n",
    "    # 1. Missing value imputation\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    X_train_imputed = imputer.fit_transform(X_train)\n",
    "    \n",
    "    # 2. Feature scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "    \n",
    "    # 3. Feature selection\n",
    "    selector = SelectKBest(f_regression, k=10)\n",
    "    X_train_selected = selector.fit_transform(X_train_scaled, y_train)\n",
    "    \n",
    "    # Return fitted preprocessors and transformed data\n",
    "    preprocessors = {\n",
    "        'imputer': imputer,\n",
    "        'scaler': scaler,\n",
    "        'selector': selector\n",
    "    }\n",
    "    \n",
    "    return X_train_selected, preprocessors\n",
    "\n",
    "def apply_preprocessing_pipeline(X_test, preprocessors):\n",
    "    \"\"\"Apply training preprocessing to test data\"\"\"\n",
    "    \n",
    "    # Apply in same order as training\n",
    "    X_test_imputed = preprocessors['imputer'].transform(X_test)\n",
    "    X_test_scaled = preprocessors['scaler'].transform(X_test_imputed)\n",
    "    X_test_selected = preprocessors['selector'].transform(X_test_scaled)\n",
    "    \n",
    "    return X_test_selected\n",
    "\n",
    "# Usage\n",
    "X_train_processed, fitted_preprocessors = create_preprocessing_pipeline(X_train, y_train)\n",
    "X_test_processed = apply_preprocessing_pipeline(X_test, fitted_preprocessors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "42d38a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data in the processed folder\n",
    "\n",
    "\n",
    "y_train.to_csv(\"../data/processed/y_train.csv\", index=False)\n",
    "y_test.to_csv(\"../data/processed/y_test.csv\", index=False)\n",
    "np.savetxt(\"../data/processed/X_train.csv\", X_train_processed, delimiter=\",\")\n",
    "np.savetxt(\"../data/processed/X_test.csv\", X_test_processed, delimiter=\",\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
