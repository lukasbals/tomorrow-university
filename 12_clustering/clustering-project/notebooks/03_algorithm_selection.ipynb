{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77931de4",
   "metadata": {},
   "source": [
    "# Clustering Algorithm Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8741239",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "\n",
    "df_unscaled = pd.read_csv(\"../data/interim/weather_per_country.csv\", index_col=0)\n",
    "df_standard_scaled = pd.read_csv(\"../data/processed/X_standard_scaled.csv\", index_col=0)\n",
    "df_tsne = pd.read_csv(\"../data/processed/X_tsne.csv\", index_col=0)\n",
    "df_umap = pd.read_csv(\"../data/processed/X_umap.csv\", index_col=0)\n",
    "\n",
    "data = {\n",
    "    \"unscaled\": df_unscaled,\n",
    "    \"standard_scaled\": df_standard_scaled,\n",
    "    \"tsne\": df_tsne,\n",
    "    \"umap\": df_umap,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26f66f2",
   "metadata": {},
   "source": [
    "## Systematic Algorithm Selection Framework\n",
    "\n",
    "Select a clustering algorithm based on the sample size and the number of features in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d9c9c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: unscaled\n",
      "ALGORITHM SELECTION FRAMEWORK\n",
      "==============================\n",
      "Data size: 185 samples (Small - all algorithms feasible)\n",
      "Features: 12 (High dimensional - consider preprocessing)\n",
      "Recommendation: Hierarchical (rich analysis)\n",
      "Hierarchical (rich analysis)\n",
      "\n",
      "Dataset: standard_scaled\n",
      "ALGORITHM SELECTION FRAMEWORK\n",
      "==============================\n",
      "Data size: 185 samples (Small - all algorithms feasible)\n",
      "Features: 10 (High dimensional - consider preprocessing)\n",
      "Recommendation: Hierarchical (rich analysis)\n",
      "Hierarchical (rich analysis)\n",
      "\n",
      "Dataset: tsne\n",
      "ALGORITHM SELECTION FRAMEWORK\n",
      "==============================\n",
      "Data size: 185 samples (Small - all algorithms feasible)\n",
      "Features: 1 (Low dimensional - no issues)\n",
      "Recommendation: Hierarchical (rich analysis)\n",
      "Hierarchical (rich analysis)\n",
      "\n",
      "Dataset: umap\n",
      "ALGORITHM SELECTION FRAMEWORK\n",
      "==============================\n",
      "Data size: 185 samples (Small - all algorithms feasible)\n",
      "Features: 1 (Low dimensional - no issues)\n",
      "Recommendation: Hierarchical (rich analysis)\n",
      "Hierarchical (rich analysis)\n"
     ]
    }
   ],
   "source": [
    "def systematic_algorithm_selection(data):\n",
    "    \"\"\"Simple framework for algorithm selection\"\"\"\n",
    "    n_samples, n_features = data.shape\n",
    "\n",
    "    print(\"ALGORITHM SELECTION FRAMEWORK\")\n",
    "    print(\"=\" * 30)\n",
    "\n",
    "    # Step 1: Data size assessment\n",
    "    if n_samples < 500:\n",
    "        size_category = \"Small - all algorithms feasible\"\n",
    "    elif n_samples < 5000:\n",
    "        size_category = \"Medium - prefer K-means or DBSCAN\"\n",
    "    else:\n",
    "        size_category = \"Large - K-means strongly preferred\"\n",
    "\n",
    "    print(f\"Data size: {n_samples} samples ({size_category})\")\n",
    "\n",
    "    # Step 2: Dimensionality assessment\n",
    "    if n_features < 10:\n",
    "        dim_category = \"Low dimensional - no issues\"\n",
    "    else:\n",
    "        dim_category = \"High dimensional - consider preprocessing\"\n",
    "\n",
    "    print(f\"Features: {n_features} ({dim_category})\")\n",
    "\n",
    "    # Step 3: Simple recommendation\n",
    "    if n_samples > 10000:\n",
    "        recommendation = \"K-means (scalable)\"\n",
    "    elif n_samples < 1000:\n",
    "        recommendation = \"Hierarchical (rich analysis)\"\n",
    "    else:\n",
    "        recommendation = \"K-means or DBSCAN (depends on cluster shapes)\"\n",
    "\n",
    "    print(f\"Recommendation: {recommendation}\")\n",
    "\n",
    "    return recommendation\n",
    "\n",
    "\n",
    "# Example usage\n",
    "for key, df in data.items():\n",
    "    print(f\"\\nDataset: {key}\")\n",
    "    print(systematic_algorithm_selection(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9639ca",
   "metadata": {},
   "source": [
    "## Data Characteristics Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52988c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: unscaled\n",
      "DATA CHARACTERISTICS ANALYSIS\n",
      "==============================\n",
      "Samples: 185\n",
      "Features: 12\n",
      "Missing data: 0.0%\n",
      "Numeric features: 12\n",
      "Categorical features: 0\n",
      "Scale difference ratio: 2950.0\n",
      "⚠️  Large scale differences - scaling recommended\n",
      "✅ Data looks good for clustering\n",
      "\n",
      "Dataset: standard_scaled\n",
      "DATA CHARACTERISTICS ANALYSIS\n",
      "==============================\n",
      "Samples: 185\n",
      "Features: 10\n",
      "Missing data: 0.0%\n",
      "Numeric features: 10\n",
      "Categorical features: 0\n",
      "Scale difference ratio: 1.6\n",
      "✅ Data looks good for clustering\n",
      "\n",
      "Dataset: tsne\n",
      "DATA CHARACTERISTICS ANALYSIS\n",
      "==============================\n",
      "Samples: 185\n",
      "Features: 1\n",
      "Missing data: 0.0%\n",
      "Numeric features: 1\n",
      "Categorical features: 0\n",
      "✅ Data looks good for clustering\n",
      "\n",
      "Dataset: umap\n",
      "DATA CHARACTERISTICS ANALYSIS\n",
      "==============================\n",
      "Samples: 185\n",
      "Features: 1\n",
      "Missing data: 0.0%\n",
      "Numeric features: 1\n",
      "Categorical features: 0\n",
      "✅ Data looks good for clustering\n"
     ]
    }
   ],
   "source": [
    "def analyze_data_characteristics(data):\n",
    "    \"\"\"Analyze key data characteristics for algorithm selection\"\"\"\n",
    "\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        df = data\n",
    "    else:\n",
    "        df = pd.DataFrame(data)\n",
    "\n",
    "    print(\"DATA CHARACTERISTICS ANALYSIS\")\n",
    "    print(\"=\" * 30)\n",
    "\n",
    "    # Basic stats\n",
    "    n_samples, n_features = df.shape\n",
    "    print(f\"Samples: {n_samples:,}\")\n",
    "    print(f\"Features: {n_features}\")\n",
    "\n",
    "    # Missing data\n",
    "    missing_pct = (df.isnull().sum().sum() / df.size) * 100\n",
    "    print(f\"Missing data: {missing_pct:.1f}%\")\n",
    "\n",
    "    # Data types\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    categorical_cols = df.select_dtypes(exclude=[np.number]).columns\n",
    "    print(f\"Numeric features: {len(numeric_cols)}\")\n",
    "    print(f\"Categorical features: {len(categorical_cols)}\")\n",
    "\n",
    "    # Scale differences (for numeric data\n",
    "    if len(numeric_cols) > 1:\n",
    "        ranges = df[numeric_cols].max() - df[numeric_cols].min()\n",
    "        max_range = ranges.max()\n",
    "        min_range = ranges[ranges > 0].min()\n",
    "        scale_ratio = max_range / min_range if min_range > 0 else 0\n",
    "        print(f\"Scale difference ratio: {scale_ratio:.1f}\")\n",
    "\n",
    "        if scale_ratio > 100:\n",
    "            print(\"⚠️  Large scale differences - scaling recommended\")\n",
    "\n",
    "    # High-level recommendations\n",
    "    issues = []\n",
    "    if missing_pct > 20:\n",
    "        issues.append(\"High missing data\")\n",
    "    if len(categorical_cols) > len(numeric_cols):\n",
    "        issues.append(\"Mostly categorical data\")\n",
    "    if n_features > 20:\n",
    "        issues.append(\"High dimensionality\")\n",
    "\n",
    "    if issues:\n",
    "        print(\"Issues to address:\", \", \".join(issues))\n",
    "    else:\n",
    "        print(\"✅ Data looks good for clustering\")\n",
    "\n",
    "    return {\n",
    "        \"n_samples\": n_samples,\n",
    "        \"n_features\": n_features,\n",
    "        \"missing_pct\": missing_pct,\n",
    "        \"n_numeric\": len(numeric_cols),\n",
    "        \"n_categorical\": len(categorical_cols),\n",
    "    }\n",
    "\n",
    "\n",
    "for key, df in data.items():\n",
    "    print(f\"\\nDataset: {key}\")\n",
    "    analyze_data_characteristics(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24afadef",
   "metadata": {},
   "source": [
    "## Computational Resource Assessment: Matching Algorithms to Available Resources\n",
    "\n",
    "We don't need to take this into account for our dataset since they are small."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28195eb",
   "metadata": {},
   "source": [
    "## Cross-Validation for Algorithm Selection\n",
    "\n",
    "### Why It Matters\n",
    "Clustering lacks ground truth labels, so traditional cross-validation doesn’t apply directly. Instead, assess stability and reproducibility across data splits and algorithm parameter settings to choose algorithms that produce consistent patterns.\n",
    "\n",
    "### Approaches:\n",
    "\n",
    "1. **Bootstrap Stability**: Resample data with replacement, cluster each sample, measure agreement (Adjusted Rand Index) between runs\n",
    "2. **Subsample Validation**: Split data into train/validation, fit on train, apply to validation (e.g., PCA+K-means)\n",
    "3. **Parameter Robustness**: Vary key parameters (K for K-means, eps for DBSCAN), check whether cluster assignments change drastically\n",
    "\n",
    "We are going to implement the bootstrap stability approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e09ccea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-means stability (ARI): 0.687\n",
      "DBSCAN stability (ARI): 0.900\n",
      "Hierarchical stability (ARI): 0.742\n"
     ]
    }
   ],
   "source": [
    "def bootstrap_stability(data, cluster_func, n_runs=10):\n",
    "    \"\"\"Assess clustering stability via bootstrap\"\"\"\n",
    "    labels_list = []\n",
    "    for i in range(n_runs):\n",
    "        # Resample indices to keep track of mapping to original data\n",
    "        sample = resample(data, random_state=i)\n",
    "        sample_indices = sample.index if hasattr(sample, \"index\") else None\n",
    "        labels = cluster_func(sample)\n",
    "        # Store both labels and indices for later alignment\n",
    "        labels_list.append((labels, sample_indices))\n",
    "    # Compute pairwise ARI\n",
    "    aris = []\n",
    "    for i in range(n_runs):\n",
    "        for j in range(i + 1, n_runs):\n",
    "            # Align labels by sorting according to sample indices\n",
    "            labels_i, idx_i = labels_list[i]\n",
    "            labels_j, idx_j = labels_list[j]\n",
    "            if idx_i is not None and idx_j is not None:\n",
    "                # Find intersection of indices and align\n",
    "                common_idx = np.intersect1d(idx_i, idx_j)\n",
    "                idx_i_aligned = [np.where(idx_i == ix)[0][0] for ix in common_idx]\n",
    "                idx_j_aligned = [np.where(idx_j == ix)[0][0] for ix in common_idx]\n",
    "                aligned_labels_i = labels_i[idx_i_aligned]\n",
    "                aligned_labels_j = labels_j[idx_j_aligned]\n",
    "                aris.append(adjusted_rand_score(aligned_labels_i, aligned_labels_j))\n",
    "            else:\n",
    "                aris.append(adjusted_rand_score(labels_i, labels_j))\n",
    "    return np.mean(aris)\n",
    "\n",
    "\n",
    "# Usage with K-means\n",
    "def km_labels(data):\n",
    "    return KMeans(n_clusters=4, random_state=42).fit_predict(data)\n",
    "\n",
    "\n",
    "# Usage with DBSCAN\n",
    "def dbscan_labels(data):\n",
    "    return DBSCAN(eps=0.5).fit_predict(data)\n",
    "\n",
    "\n",
    "# Usage with Hierarchical (Agglomerative)\n",
    "def hierarchical_labels(data):\n",
    "    return AgglomerativeClustering(n_clusters=4).fit_predict(data)\n",
    "\n",
    "\n",
    "k_means_stability = bootstrap_stability(data[\"umap\"], km_labels, n_runs=20)\n",
    "print(f\"K-means stability (ARI): {k_means_stability:.3f}\")\n",
    "\n",
    "dbscan_stability = bootstrap_stability(data[\"umap\"], dbscan_labels, n_runs=20)\n",
    "print(f\"DBSCAN stability (ARI): {dbscan_stability:.3f}\")\n",
    "\n",
    "hierarchical_stability = bootstrap_stability(\n",
    "    data[\"umap\"], hierarchical_labels, n_runs=20\n",
    ")\n",
    "print(f\"Hierarchical stability (ARI): {hierarchical_stability:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8354c4e",
   "metadata": {},
   "source": [
    "## Cross validation insights\n",
    "\n",
    "<!-- K-means stability (ARI): 0.687\n",
    "DBSCAN stability (ARI): 0.900\n",
    "Hierarchical stability (ARI): 0.742 -->\n",
    "\n",
    "This is a interesting result and after investigating I found, that DBSCAN only seems stable because in most of the cases it clusters into one big cluster. This is not a useful clustering result which leads me to the conclusion that K-means and Hierarchical clustering make sense for our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8874049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-means: Silhouette=0.587, DB-Index=0.492\n",
      "DBSCAN: Silhouette=-1.000, DB-Index=inf\n",
      "Hierarchical: Silhouette=0.572, DB-Index=0.484\n",
      "Best algorithm by DB-Index: Hierarchical\n"
     ]
    }
   ],
   "source": [
    "def evaluate_algorithms(data, algorithms):\n",
    "    \"\"\"Evaluate multiple clustering algorithms on key metrics\"\"\"\n",
    "    results = {}\n",
    "    for name, labels in algorithms.items():\n",
    "        if len(set(labels)) > 1:\n",
    "            sil = silhouette_score(data, labels)\n",
    "            db = davies_bouldin_score(data, labels)\n",
    "        else:\n",
    "            sil, db = -1, np.inf\n",
    "        results[name] = {\"silhouette\": sil, \"db_index\": db}\n",
    "        print(f\"{name}: Silhouette={sil:.3f}, DB-Index={db:.3f}\")\n",
    "    return results\n",
    "\n",
    "\n",
    "# Example usage\n",
    "alg_results = {\n",
    "    \"K-means\": KMeans(n_clusters=4).fit_predict(data[\"umap\"]),\n",
    "    \"DBSCAN\": DBSCAN(eps=0.5).fit_predict(data[\"umap\"]),\n",
    "    \"Hierarchical\": AgglomerativeClustering(n_clusters=4).fit_predict(data[\"umap\"]),\n",
    "}\n",
    "metrics = evaluate_algorithms(data[\"umap\"], alg_results)\n",
    "best = min(metrics, key=lambda x: metrics[x][\"db_index\"])\n",
    "print(f\"Best algorithm by DB-Index: {best}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f96507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RUNNING BASIC CLUSTERING SUITE\n",
      "------------------------------\n",
      "✅ K-means (K=4): Silhouette = 0.597\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "DBSCAN.__init__() got an unexpected keyword argument 'n_clusters'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m     34\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m basic_results = \u001b[43mrun_basic_clustering_suite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mumap\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mrun_basic_clustering_suite\u001b[39m\u001b[34m(scaled_data)\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✅ K-means (K=4): Silhouette = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkm_score\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# DBSCAN with reasonable defaults\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m dbscan = \u001b[43mDBSCAN\u001b[49m\u001b[43m(\u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_clusters\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m db_labels = dbscan.fit_predict(scaled_data)\n\u001b[32m     19\u001b[39m db_n_clusters = \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(db_labels)) - (\u001b[32m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m -\u001b[32m1\u001b[39m \u001b[38;5;129;01min\u001b[39;00m db_labels \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: DBSCAN.__init__() got an unexpected keyword argument 'n_clusters'"
     ]
    }
   ],
   "source": [
    "def run_basic_clustering_suite(scaled_data):\n",
    "    \"\"\"Run basic version of each algorithm for initial assessment\"\"\"\n",
    "\n",
    "    print(\"\\nRUNNING BASIC CLUSTERING SUITE\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # K-means with K=4 (common starting point)\n",
    "    kmeans = KMeans(n_clusters=4, random_state=42)\n",
    "    km_labels = kmeans.fit_predict(scaled_data)\n",
    "    km_score = silhouette_score(scaled_data, km_labels)\n",
    "    results[\"kmeans\"] = {\"labels\": km_labels, \"silhouette\": km_score, \"n_clusters\": 4}\n",
    "    print(f\"✅ K-means (K=4): Silhouette = {km_score:.3f}\")\n",
    "\n",
    "    # DBSCAN with reasonable defaults\n",
    "    dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "    db_labels = dbscan.fit_predict(scaled_data)\n",
    "    db_n_clusters = len(set(db_labels)) - (1 if -1 in db_labels else 0)\n",
    "    if db_n_clusters > 1:\n",
    "        db_score = silhouette_score(scaled_data, db_labels)\n",
    "    else:\n",
    "        db_score = -1\n",
    "    results[\"dbscan\"] = {\n",
    "        \"labels\": db_labels,\n",
    "        \"silhouette\": db_score,\n",
    "        \"n_clusters\": db_n_clusters,\n",
    "    }\n",
    "    print(f\"✅ DBSCAN: {db_n_clusters} clusters, Silhouette = {db_score:.3f}\")\n",
    "\n",
    "    # Hierarchical with K=4\n",
    "    hier = AgglomerativeClustering(n_clusters=4)\n",
    "    hier_labels = hier.fit_predict(scaled_data)\n",
    "    hier_score = silhouette_score(scaled_data, hier_labels)\n",
    "    results[\"hierarchical\"] = {\n",
    "        \"labels\": hier_labels,\n",
    "        \"silhouette\": hier_score,\n",
    "        \"n_clusters\": 4,\n",
    "    }\n",
    "    print(f\"✅ Hierarchical (K=4): Silhouette = {hier_score:.3f}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Example usage\n",
    "basic_results = run_basic_clustering_suite(data[\"umap\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
